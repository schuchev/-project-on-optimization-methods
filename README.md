# Проект по численным методам оптимизации
Имплементация статьи https://arxiv.org/pdf/2206.06900 , в которой реализован новый метод оптимизации GradaGrad, который является улучшением метода AdaGrad за счет немонотонного изменения learning rate. 

В папке optimizer реализован как раз метод GradaGrad и упрощенная его скалярная версия, а также другие известные методы: Adam,Adagrad,SGD.

В файле optimizer_comprasion.ipynb реализовано сравнение GradaGrad с оптимизаторами Adam, SGD и AdaGrad на 6 датасетах из LIBSVM (Glass, Letter и др.) для задачи бинарной логистической регрессии. Проведен автоматический подбор learning rate через grid search и 10 запусков с разными random seeds для каждого метода. Результаты визуализированы с доверительными интервалами, показывая, что GradaGrad достигает сопоставимой точности с фиксированными γ₀=1.0 и ρ=2.

Файл gradagrad_vs_adagrad.ipynb визуализирует ключевое отличие GradaGrad от AdaGrad на примере оптимизации функции |x|. На графиках показано:
Динамика learning rate (не монотонная у GradaGrad vs монотонное уменьшение у AdaGrad)
Траектория сходимости (быстрый выход из плохого начального learning rate у GradaGrad)
Сравнение потерь во времени, иллюстрирующее ускоренную сходимость благодаря адаптивности.

В файле DLRM.ipynb была попытка использовать упрощуенную версию GradaGrad на модели DLRM на датасете Display Advertising Challenge. Ввиду сложности обучения мы брали упрощенный вариант датасета (100000 строк) и упрощенную модель DLRM. Все было сделано как в статье, но появились проблемы во время обучения: при некоторых параметрах возникали Nan, а при других learning rate быстро падал к очень маленькому значению и не менялся, пытались решить различными способами, но не получилось, возможно из-за упрощенной версии.
