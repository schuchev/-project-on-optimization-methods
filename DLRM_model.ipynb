{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a593a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self, self_interaction):\n",
    "        super().__init__()\n",
    "        self.self_interaction = self_interaction\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature_dim = inputs.shape[1]\n",
    "        concat_features = inputs.view(-1, feature_dim, 1)\n",
    "        dot_products = torch.matmul(concat_features, concat_features.transpose(1, 2))\n",
    "        ones = torch.ones_like(dot_products)\n",
    "        mask = torch.triu(ones)\n",
    "        out_dim = feature_dim * (feature_dim + 1) // 2\n",
    "        flat_result = dot_products[mask.bool()]\n",
    "        reshape_result = flat_result.view(-1, out_dim)\n",
    "        return reshape_result\n",
    "\n",
    "class DLRM(nn.Module):\n",
    "    def __init__(self, sparse_feature_number, dense_feature_number, num_embeddings, embed_dim, bottom_mlp_dims, top_mlp_dims, self_interaction):\n",
    "        super(DLRM, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sparse_feature_number = sparse_feature_number\n",
    "        self.bottom_mlp_output_dim = bottom_mlp_dims[-1]\n",
    "        self.embedding = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.layer_feature_interaction = FeatureInteraction(self_interaction)\n",
    "        self.bottom_mlp = nn.Sequential(\n",
    "            nn.Linear(dense_feature_number, bottom_mlp_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(bottom_mlp_dims[0], bottom_mlp_dims[1]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        feature_interaction_input_dim = self.bottom_mlp_output_dim + (sparse_feature_number * embed_dim)\n",
    "        interaction_output_dim = (feature_interaction_input_dim * (feature_interaction_input_dim + 1)) // 2\n",
    "        input_dim_for_top_mlp = interaction_output_dim + self.bottom_mlp_output_dim\n",
    "        self.top_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim_for_top_mlp, top_mlp_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[0], top_mlp_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        embed_x = self.embedding(x_sparse)\n",
    "        embed_x = embed_x.view(x_sparse.shape[0], -1)\n",
    "        bottom_mlp_output = self.bottom_mlp(x_dense)\n",
    "        concat_first = torch.cat([bottom_mlp_output, embed_x], dim=-1)\n",
    "        interaction = self.layer_feature_interaction(concat_first)\n",
    "        concat_second = torch.cat([interaction, bottom_mlp_output], dim=-1)\n",
    "        output = self.top_mlp(concat_second)\n",
    "        return output.squeeze().unsqueeze(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Updated)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
